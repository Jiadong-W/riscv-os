# 实验七：文件系统

## 一、实验目的

本实验对应指导手册“实验7：文件系统”。目标是在已有块设备驱动（VirtIO）、缓冲层（bio）、日志层（log）基础上，完成一个**可用且具备崩溃一致性（crash-consistency）**的简化日志文件系统，并理解其关键设计点。具体目标：

1. 理解文件系统的层次结构：块设备 → 缓冲缓存（buffer cache）→ 日志（write-ahead log）→ inode/目录 → 文件描述符接口。
2. 掌握 inode、目录项与路径解析的核心实现：`iget/ilock/iupdate/bmap/readi/writei/namei/dirlink` 等。
3. 实现块分配与回收：位图管理 + 数据块/索引块分配（支持大文件的间接块）。
4. 实现写前日志（WAL）与事务：`begin_transaction/end_transaction/log_block_write/commit/recover`，保证文件系统元数据与数据更新的原子性。
5. 理解并实现磁盘 I/O 路径：`virtio_disk.c` 提供块读写 → `bio.c` 缓冲与锁 → 上层文件系统使用 `bread/bwrite/brelse`。

> 说明：本报告按“加强版”要求编写：去掉“实验环境”章节，增加源码级分析，并保留测试截图占位；LaTeX 版本包含并完成实验七“思考题”。

---

## 二、实验原理

### 2.1 文件系统的基本抽象：块、inode、目录与文件

- **块（block）**：磁盘访问的最小单位，通常是 512B/1KB/4KB。文件系统把磁盘组织成固定大小块，所有数据结构（超级块、inode 表、位图、目录、文件数据）都以块为单位存放。
- **inode**：文件的元数据对象，记录文件类型、大小、数据块地址（直接块/间接块）、链接计数等。目录本质上也是一种文件，其内容是“目录项数组”（name → inum）。
- **路径解析**：把 `/a/b/c` 逐段解析，依次在目录中查找目录项，得到对应 inode。

### 2.2 为什么需要 Buffer Cache（bio）

直接对磁盘进行“裸读写”会非常慢且难以并发控制。缓冲层提供：

1. **缓存命中**：热点块重复访问可直接命中内存，减少 I/O。
2. **一致的锁语义**：对每个缓存块使用睡眠锁，避免并发修改导致数据混乱。
3. **写回策略/脏块管理**：把“修改”延后到合适时机写回磁盘；配合日志层实现 crash-consistency。

### 2.3 写前日志（Write-Ahead Logging, WAL）与崩溃一致性

没有日志时，文件系统一次更新通常涉及多个块（例如：写数据块 + 更新 inode + 更新位图 + 更新目录项）。若中途崩溃，可能出现：

- 位图已分配但 inode 未更新（泄漏）
- inode 指向的数据块未写完（脏数据）
- 目录项已出现但 inode 仍旧无效（悬挂引用）

WAL 的关键思想：**先把本次事务涉及的“修改后的块内容”写入日志区，再一次性“安装（install）”到真实位置**。这样：

- 崩溃发生在“写日志之前” → 无影响（未提交）
- 崩溃发生在“写日志之后、安装之前” → 重启时根据日志重放即可
- 崩溃发生在“安装过程中” → 重启时仍可重放，直到完成

因此日志为“事务”提供了原子性：要么全部生效，要么全部不生效。

### 2.4 磁盘 I/O：VirtIO 块设备驱动（virtio_disk）

QEMU 提供 VirtIO 块设备；驱动通过 virtio-mmio 寄存器协商特性、初始化 virtqueue，使用描述符（descriptor）提交 I/O 请求，并等待设备完成。上层文件系统只需要 `disk_rw(buf, write)` 这样的统一接口。

---

## 三、实验内容与实现（结合源码分析）

本实验关键源码文件：

- `virtio_disk.c`：VirtIO 磁盘驱动（块设备层）
- `bio.c`：buffer cache（块缓存层）
- `log.c`：写前日志与事务（WAL 层）
- `fs.c`：文件系统核心（superblock/inode/目录/块分配/路径解析）
- `file.c`：打开文件表与读写接口（file descriptor 层）

下面按“自底向上”梳理完整数据路径。

---

### 3.1 VirtIO 块设备驱动：virtio_disk.c

#### (1) 特性协商与设备初始化

驱动初始化阶段读取 `VIRTIO_MMIO_DEVICE_FEATURES`，屏蔽不需要的特性（只读、SCSI、多队列、间接描述符等），然后写回 `VIRTIO_MMIO_DRIVER_FEATURES` 并置位 `FEATURES_OK`：

- 这样做的意义是：只启用本实验需要的最小特性集合，降低驱动复杂度，便于在尚未完善中断体系时采用轮询方式完成 I/O。

#### (2) virtqueue 与描述符组织

VirtIO 块请求通常使用 3 个描述符链：

1. **请求头**（包含读/写类型、扇区号）
2. **数据缓冲区**（读：设备写入；写：设备读取）
3. **状态字节**（设备写入完成状态）

驱动把这些描述符压入可用环（avail ring），写 `QUEUE_NOTIFY` 通知设备，然后轮询已用环（used ring）等待完成。

#### (3) 忙等轮询（polling）模式的权衡

代码注释明确说明：实现“精简为忙等轮询模式”，便于在尚未接入复杂中断控制时工作。缺点是：

- I/O 期间 CPU 会被占用（吞吐下降）
- 但在教学内核、单核环境下可接受，且更容易调试

---

### 3.2 Buffer Cache：bio.c（按块缓存 + 锁 + LRU）

#### (1) 缓存结构：哈希桶 + LRU 链表

本实现使用：

- `NBUF=32` 个 `struct buf` 作为缓存块；
- 哈希表 `BUF_HASH_SIZE=37` 降低冲突；
- 双向链表维护近似 LRU：释放时把空闲块移到头部，提升再次命中概率。

#### (2) 核心接口：bread/bwrite/brelse

- `bread(dev, blockno)`：返回一个上锁（sleep lock）的缓存块；若缓存未命中则从磁盘读入。
- `bwrite(b)`：把缓存块标记为脏并写回磁盘（底层调用 `disk_rw(b, 1)`）。
- `brelse(b)`：释放 sleep lock，并更新 refcnt 与 LRU 位置；refcnt 降为 0 时该块可复用。

关键工程点：

- **块级睡眠锁**：保证同一块的并发读写互斥，避免“目录项半更新”“inode 写一半”等灾难性错误。
- **引用计数 refcnt**：允许多个调用者共享同一缓存块（例如路径解析时多次读取同一目录块），避免重复分配。

---

### 3.3 日志层：log.c（事务 begin/end + log write + commit + recover）

该实现遵循 xv6 的 WAL 思路，函数命名为“transaction”：

- `log_init(dev, sb)`：读取超级块中日志区信息并初始化日志子系统
- `begin_transaction()` / `end_transaction()`：系统调用级事务边界
- `log_block_write(bp)`：记录“本事务修改过的块”
- `commit_transaction()`：写日志块 → 写日志头 → 安装到目标块 → 清空日志头
- `recover_log()`：启动时重放日志，保证崩溃恢复

#### (1) 并发与日志空间：outstanding + committing

`begin_transaction()` 里有两个关键等待条件：

1. `committing` 为真：正在提交，新的事务必须等待；
2. “预估空间不足”：`header.n + (outstanding+1)*MAX_OP_BLOCKS > log.size` 时等待提交释放空间。

这体现了日志系统的核心约束：**日志区容量有限**，且一个系统调用（事务）可能修改多个块，必须预留最坏情况空间避免“写到一半发现日志满了”。

#### (2) log_block_write：去重记录 + 延迟写回

日志系统通常只记录“块号列表”，并将“块内容”在提交时写入日志区。实现要点：

- 同一事务内重复修改同一块，需要去重（否则浪费日志空间）
- `B_DIRTY`/标记用于让缓冲层知道“该块必须被提交逻辑处理”

#### (3) commit_transaction：提交的 3 个阶段

提交通常分为三步：

1. `write_log_blocks()`：把修改后的块内容写入日志区对应位置（log area）
2. `write_log_header()`：写入日志头（包含 n 和 block[]），这是“提交点”
3. `install_transaction(recovering)`：把日志区数据复制到真正目标块（home location）
4. 清空日志头（n=0）写回，表示日志已完成

这套顺序保证了原子性：**以“日志头写入成功”为事务是否提交的判据**。

#### (4) recover_log：崩溃恢复为什么可行

启动时执行：

- 读日志头
- 若 `n > 0` 说明上次崩溃时存在已提交但可能未安装完成的事务
- 执行 `install_transaction(recovering=1)` 重放日志
- 清空日志头

即使在恢复过程中再次崩溃，重启后仍能重复同样流程，直到安装完成，具备幂等性（重复安装不会把正确数据变坏）。

---

### 3.4 文件系统核心：fs.c（superblock/inode/目录/位图/路径解析）

#### (1) fs_init：超级块初始化与布局

`fs_init(dev)` 会读取/构造超级块信息（如 inode 区、位图区、数据区、日志区起始与大小），并初始化 inode 缓存表（itable）。超级块决定了“磁盘上各类结构的块号范围”，是整个文件系统的全局元数据入口。

#### (2) inode 缓存：iget/ilock/iupdate/iunlock

- `iget(dev, inum)`：在内存 inode 缓存中查找（dev,inum），命中则 ref++；未命中则找 ref==0 的槽位初始化（valid=0 懒加载）。
- `ilock(ip)`：若 inode 未 valid，则从磁盘 inode 表读入并解析到内存结构；随后获取 sleep lock 防止并发修改。
- `iupdate(ip)`：把内存 inode 元数据写回磁盘 inode 表（属于元数据更新，通常要在事务内完成）。
- `iunlock(ip)`：释放 inode 的 sleep lock。

这种“缓存 + 懒加载”策略避免频繁读盘，同时保证 inode 修改有明确互斥边界。

#### (3) 块分配与回收：balloc/bfree + 位图

- `balloc(dev)`：在位图中寻找空闲 bit，置 1 并返回对应数据块号；通常还会把新块清零以避免泄露旧数据。
- `bfree(dev, b)`：将位图 bit 清零，回收块。

位图更新必须在事务中，避免崩溃造成“块既被 inode 引用又显示空闲”的双重分配。

#### (4) 文件数据映射：bmap（直接块 + 间接块）

`bmap(ip, bn)` 将“文件内第 bn 个数据块”映射到“磁盘块号”，并在需要时分配：

- 小 bn：使用直接块数组（速度快）
- 大 bn：使用间接块（indirect），间接块本身也是磁盘块，保存更多数据块地址

这一机制决定了文件最大大小与寻址开销：直接块快但数量有限，间接块扩展容量但多一次读盘。

#### (5) 目录与路径：dirlookup/dirlink/namei/nameiparent

- `dirlookup(dp, name, &off)`：遍历目录文件内容，查找匹配目录项，返回目标 inode（或 inum）。
- `dirlink(dp, name, inum)`：在目录中添加目录项，必要时扩展目录文件大小。
- `namei(path)`：按路径分段解析，逐级 `dirlookup`，最终得到 inode。
- `nameiparent(path, name)`：返回父目录 inode，并把最后一级名字写入 name（常用于 create/unlink）。

这套逻辑与系统调用层（`sysfile.c`）密切相关：open/create/unlink/chdir 都依赖它。

#### (6) readi/writei：在 inode 上做偏移读写

`readi(ip, dst, off, n)` 与 `writei(ip, src, off, n)` 的共同点：

- 根据 off 与 n 计算需要访问哪些文件块
- 对每个块调用 `bmap` 找到磁盘块号，再 `bread` 读入缓存块
- copy 到目标缓冲区（内核缓冲或用户缓冲的 copyin/out 在更上层完成）
- 写入时需要标记块脏并通过 `log_block_write` 纳入事务

特别注意：写入数据块也走日志，意味着数据与元数据都可获得原子性（简化但安全）。

---

### 3.5 打开文件表：file.c（fd 层与事务分批）

`file.c` 管理全局 `struct file file[NFILE]` 与每进程 `ofile[NOFILE]` 指针表，负责把“系统调用的 fd”映射到“inode/设备/管道对象”。

#### (1) devsw：设备读写回调表

`devsw[NDEV]` 将设备主编号映射到 `read/write` 回调。文件系统把设备文件当作特殊 inode，最终通过 `devsw` 进入驱动层（例如 console 或块设备）。

#### (2) filewrite 的“分批写入”为什么必要

`filewrite()` 中有关键注释：

> 分批写入以避免单次事务占用过多日志块，公式与 xv6 保持一致。

因为日志空间有限，一个 write(n) 可能触发：

- 写数据块
- 更新 inode
- 更新位图
- 更新间接块
- 更新目录（某些情况下）

如果一次 write 写太大，可能超出 `MAX_OP_BLOCKS` 的事务上限，导致日志空间预估失败。解决方案是：

- 把大写入拆成多个 chunk
- 每个 chunk 用 `begin_transaction/end_transaction` 包起来
- 保证每次提交的块数有上界，不会把日志“撑爆”

这体现了日志文件系统常见的工程约束：**事务必须有界**。

---

## 四、实验步骤

1. 阅读实验7手册：理解文件系统布局、inode/目录/位图、以及日志（WAL）基本概念。
2. 从底层开始阅读代码：
   1) `virtio_disk.c`：确认块读写路径与请求结构  
   2) `bio.c`：理解 bread/bwrite/brelse 的锁与缓存语义  
   3) `log.c`：理解事务 begin/end、commit 与 recover 的顺序  
   4) `fs.c`：理解 inode/目录/路径/块分配与 bmap  
   5) `file.c`：理解 fd 层、事务分批写入
3. 编译运行内核，执行文件系统测试用例：
   - 创建/读写/删除文件
   - 目录与路径解析
   - 大文件与间接块
   - 崩溃恢复（日志重放）
4. 根据测试输出与截图补全报告“实验结果与测试”部分。

---

## 五、实验结果与测试

make qemu之后，等待实验5的进程测试运行完之后，输入命令“fstest”即会运行文件系统测试

![](./实验七截图.png)

---

## 六、实验分析与问题讨论

1. **为什么日志头是“提交点”**  
   写日志块本身并不代表事务已提交；只有日志头（n 与 block[]）成功落盘，恢复时才能识别并重放。因此日志头落盘是关键原子点。
2. **事务大小上界的重要性**  
   `filewrite()` 分批写入体现：日志容量与 `MAX_OP_BLOCKS` 必须约束一次系统调用的修改量，否则会出现“写一半发现日志满了”的不可恢复状态。
3. **buffer cache 与日志的协作**  
   缓冲层负责缓存与锁，日志层负责“哪些块被修改、如何提交”。在实现上常见策略是：写路径先修改缓存块，再 `log_block_write(bp)` 把该块加入事务，由 commit 统一写盘。
4. **目录查找性能瓶颈**  
   `dirlookup` 通常是线性扫描目录项，目录大时会明显变慢。现代文件系统会使用 B+tree/HTree/哈希目录来加速查找。
5. **轮询 I/O 的代价**  
   virtio_disk 轮询简单但浪费 CPU。接入中断后可显著提升 I/O 并发与整体吞吐。

---

## 七、实验总结与心得体会

实验七把“磁盘上的持久化数据结构”真正串起来了：从 VirtIO 块设备到 buffer cache，再到 WAL 日志保证崩溃一致性，最后到 inode/目录/路径解析提供用户可见的文件接口。最有收获的是日志提交顺序与事务大小上界这两个工程约束：它们看似繁琐，但正是现代文件系统可靠性的基础。通过对 `log.c`、`fs.c` 与 `file.c` 的联动阅读，我也更清楚系统调用层（实验六）中 `begin_transaction/end_transaction` 的意义——它并不是“可有可无的包装”，而是保证文件系统一致性的边界。

